{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9884aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Python Libraries\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Dropout, Flatten\n",
    "import os\n",
    "from aksharamukha import transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7f8b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Segmentation\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_path = \"Segmentation/rednoise.jpeg\"\n",
    "\n",
    "img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "hProj = np.sum(img,1)\n",
    "\n",
    "def findPeakRegions(hpp, divider=1.02):\n",
    "    threshold = (np.max(hpp)-np.min(hpp))/divider\n",
    "    peaks = []\n",
    "    peaks_index = []\n",
    "    for i, hppv in enumerate(hpp):\n",
    "        if hppv > threshold:\n",
    "            peaks.append([i, hppv])\n",
    "    return peaks\n",
    "\n",
    "peaks = findPeakRegions(hProj)\n",
    "\n",
    "peaksIndex = np.array(peaks)[:,0].astype(int)\n",
    "\n",
    "segmentedImg = np.copy(img)\n",
    "r,c = segmentedImg.shape\n",
    "\n",
    "for ri in range(r):\n",
    "    if ri in peaksIndex:\n",
    "        segmentedImg[ri, :] = 0     \n",
    "\n",
    "hProjLines = np.sum(segmentedImg,1)\n",
    "hProjLines = np.append(hProjLines,[0,0,0])\n",
    "\n",
    "lines = []\n",
    "lIndexB = []\n",
    "lIndexE = []\n",
    "\n",
    "for ri in range(len(hProjLines)):\n",
    "    if hProjLines[ri]!=0 and hProjLines[ri-1]==0 and hProjLines[ri-2]==0:\n",
    "        lIndexB.append(ri)\n",
    "    if hProjLines[ri]!=0 and hProjLines[ri+1]==0 and hProjLines[ri+2]==0:\n",
    "        lIndexE.append(ri)\n",
    "        \n",
    "for i in range(len(lIndexB)):\n",
    "    lines.append(img[lIndexB[i]:lIndexE[i],:])\n",
    "    cv.imwrite(\"Segmentation/lines/line{}.jpg\".format(i+1),lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "989bab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 36B1-37D5\n",
      "\n",
      " Directory of C:\\Users\\rohai\\OneDrive\\Desktop\\SIH2022-Finals-Vigrah\n",
      "\n",
      "26-08-2022  00:11    <DIR>          .\n",
      "24-08-2022  22:48    <DIR>          ..\n",
      "24-08-2022  20:53    <DIR>          .ipynb_checkpoints\n",
      "26-08-2022  00:11           148,597 Get_Preds.ipynb\n",
      "26-08-2022  00:04    <DIR>          model_files\n",
      "23-08-2022  19:23                23 README.md\n",
      "26-08-2022  00:01    <DIR>          Segmentation\n",
      "               2 File(s)        148,620 bytes\n",
      "               5 Dir(s)  23,991,275,520 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ce10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['a',\n",
    " 'ba',\n",
    " 'be',\n",
    " 'bha',\n",
    " 'bhe',\n",
    " 'bhi',\n",
    " 'bho',\n",
    " 'bhu',\n",
    " 'bhÄ',\n",
    " 'bhÄ«',\n",
    " 'bhÅ«',\n",
    " 'bi',\n",
    " 'bo',\n",
    " 'bu',\n",
    " 'bÄ',\n",
    " 'ca',\n",
    " 'ce',\n",
    " 'cha',\n",
    " 'che',\n",
    " 'chi',\n",
    " 'chu',\n",
    " 'chÄ',\n",
    " 'ci',\n",
    " 'co',\n",
    " 'cu',\n",
    " 'cÄ',\n",
    " 'cÅ«',\n",
    " 'da',\n",
    " 'de',\n",
    " 'dha',\n",
    " 'dhe',\n",
    " 'dhi',\n",
    " 'dho',\n",
    " 'dhu',\n",
    " 'dhÄ',\n",
    " 'dhÄ«',\n",
    " 'dhÅ«',\n",
    " 'di',\n",
    " 'do',\n",
    " 'du',\n",
    " 'dÄ',\n",
    " 'dÄ«',\n",
    " 'e',\n",
    " 'ga',\n",
    " 'ge',\n",
    " 'gha',\n",
    " 'ghe',\n",
    " 'gho',\n",
    " 'ghu',\n",
    " 'ghÄ',\n",
    " 'gi',\n",
    " 'go',\n",
    " 'gu',\n",
    " 'gÄ',\n",
    " 'ha',\n",
    " 'he',\n",
    " 'hi',\n",
    " 'ho',\n",
    " 'hu',\n",
    " 'hÄ',\n",
    " 'hÄ«',\n",
    " 'hÅ«',\n",
    " 'i',\n",
    " 'ja',\n",
    " 'je',\n",
    " 'jha',\n",
    " 'jhi',\n",
    " 'jhÄ',\n",
    " 'ji',\n",
    " 'jo',\n",
    " 'ju',\n",
    " 'jÄ',\n",
    " 'jÄ«',\n",
    " 'jÅ«',\n",
    " 'ka',\n",
    " 'ke',\n",
    " 'kha',\n",
    " 'khe',\n",
    " 'khi',\n",
    " 'kho',\n",
    " 'khu',\n",
    " 'khÄ',\n",
    " 'khÄ«',\n",
    " 'ki',\n",
    " 'ko',\n",
    " 'ku',\n",
    " 'kÄ',\n",
    " 'kÄ«',\n",
    " 'kÅ«',\n",
    " 'la',\n",
    " 'le',\n",
    " 'li',\n",
    " 'lo',\n",
    " 'lu',\n",
    " 'lÄ',\n",
    " 'lÄ«',\n",
    " 'lÅ«',\n",
    " 'ma',\n",
    " 'me',\n",
    " 'mi',\n",
    " 'mo',\n",
    " 'mu',\n",
    " 'mÄ',\n",
    " 'mÄ«',\n",
    " 'mÅ«',\n",
    " 'na',\n",
    " 'ne',\n",
    " 'ni',\n",
    " 'no',\n",
    " 'nu',\n",
    " 'nÄ',\n",
    " 'nÄ«',\n",
    " 'nÅ«',\n",
    " 'o',\n",
    " 'pa',\n",
    " 'pe',\n",
    " 'pha',\n",
    " 'phe',\n",
    " 'phÄ',\n",
    " 'pi',\n",
    " 'po',\n",
    " 'pu',\n",
    " 'pÄ',\n",
    " 'pÄ«',\n",
    " 'ra',\n",
    " 're',\n",
    " 'ri',\n",
    " 'ro',\n",
    " 'ru',\n",
    " 'rÄ',\n",
    " 'rÄ«',\n",
    " 'rÅ«',\n",
    " 'sa',\n",
    " 'se',\n",
    " 'si',\n",
    " 'so',\n",
    " 'su',\n",
    " 'sÄ',\n",
    " 'sÄ«',\n",
    " 'sÅ«',\n",
    " 'ta',\n",
    " 'te',\n",
    " 'tha',\n",
    " 'the',\n",
    " 'thi',\n",
    " 'thu',\n",
    " 'thÄ',\n",
    " 'thÄ«',\n",
    " 'ti',\n",
    " 'to',\n",
    " 'tu',\n",
    " 'tÄ',\n",
    " 'tÄ«',\n",
    " 'tÅ«',\n",
    " 'u',\n",
    " 'va',\n",
    " 've',\n",
    " 'vi',\n",
    " 'vo',\n",
    " 'vu',\n",
    " 'vÄ',\n",
    " 'vÄ«',\n",
    " 'vÅ«',\n",
    " 'ya',\n",
    " 'ye',\n",
    " 'yi',\n",
    " 'yo',\n",
    " 'yu',\n",
    " 'yÄ',\n",
    " 'yÄ«',\n",
    " 'yÅ«',\n",
    " 'Ã±a',\n",
    " 'Ã±e',\n",
    " 'Ã±o',\n",
    " 'Ã±Ä',\n",
    " 'Ä',\n",
    " 'Å›a',\n",
    " 'Å›i',\n",
    " 'Å›u',\n",
    " 'Å›Ä',\n",
    " 'á¸a',\n",
    " 'á¸e',\n",
    " 'á¸ha',\n",
    " 'á¸he',\n",
    " 'á¸hi',\n",
    " 'á¸hÄ',\n",
    " 'á¸hÄ«',\n",
    " 'á¸i',\n",
    " 'á¸u',\n",
    " 'á¸Ä',\n",
    " 'á¸Ä«',\n",
    " 'á¹‡a',\n",
    " 'á¹‡e',\n",
    " 'á¹‡i',\n",
    " 'á¹‡Ä',\n",
    " 'á¹‡Ä«',\n",
    " 'á¹£a',\n",
    " 'á¹£e',\n",
    " 'á¹£i',\n",
    " 'á¹£o',\n",
    " 'á¹£u',\n",
    " 'á¹£Ä',\n",
    " 'á¹­a','á¹­e','á¹­ha','á¹­he','á¹­hi','á¹­hÄ','á¹­hÄ«','á¹­hÅ«','á¹­i','á¹­u','á¹­Ä','á¹­Ä«']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d1f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "brahmi_dict = {'a':'ğ‘€…','Ä':'ğ‘€†','i':'ğ‘€‡','Ä«':'ğ‘€ˆ','u':'ğ‘€‰','Å«':'ğ‘€Š','e':'ğ‘€','ai':'ğ‘€','o':'ğ‘€‘','au':'ğ‘€’','aá¹ƒ':'ğ‘€…ğ‘€',\n",
    "'ka':'ğ‘€“','kÄ':'ğ‘€“ğ‘€¸','ki':'ğ‘€“ğ‘€º','kÄ«':'ğ‘€“ğ‘€»','ku':'ğ‘€“ğ‘€¼','kÅ«':'ğ‘€“ğ‘€½','ke':'ğ‘€“ğ‘‚','kai':'ğ‘€“ğ‘ƒ','ko':'ğ‘€“ğ‘„','kau':'ğ‘€“ğ‘…','kaá¹ƒ':'ğ‘€“ğ‘€',\n",
    "'kha':'ğ‘€”â€‹','khÄ':'ğ‘€”ğ‘€¸','khi':'ğ‘€”ğ‘€º','khÄ«':'ğ‘€”ğ‘€»','khu':'ğ‘€”ğ‘€¼','khÅ«':'ğ‘€”ğ‘€½','khe':'ğ‘€”ğ‘‚','khai':'ğ‘€”ğ‘ƒ','kho':'ğ‘€”ğ‘„','khau':'ğ‘€”ğ‘…','khaá¹ƒ':'ğ‘€”ğ‘€',\n",
    "'ga':'ğ‘€•â€‹','gÄ':'ğ‘€•ğ‘€¸','gi':'ğ‘€•ğ‘€º','gÄ«':'ğ‘€•ğ‘€»','gu':'ğ‘€•ğ‘€¼','gÅ«':'ğ‘€•ğ‘€½','ge':'ğ‘€•ğ‘‚','gai':'ğ‘€•ğ‘ƒ','go':'ğ‘€•ğ‘„','gau':'ğ‘€•ğ‘…','gaá¹ƒ':'ğ‘€•ğ‘€',\n",
    "'gha':'ğ‘€–â€‹','ghÄ':'ğ‘€–ğ‘€¸','ghi':'ğ‘€–ğ‘€º','ghÄ«':'ğ‘€–ğ‘€»','ghu':'ğ‘€–ğ‘€¼','ghÅ«':'ğ‘€–ğ‘€½','ghe':'ğ‘€–ğ‘‚','ghai':'ğ‘€–ğ‘ƒ','gho':'ğ‘€–ğ‘„','ghau':'ğ‘€–ğ‘…','ghaá¹ƒ':'ğ‘€–ğ‘€',\n",
    "'á¹…a':'ğ‘€—â€‹','á¹…Ä':'ğ‘€—ğ‘€¸','á¹…i':'ğ‘€—ğ‘€º','á¹…Ä«':'ğ‘€—ğ‘€»','á¹…u':'ğ‘€—ğ‘€¼','á¹…Å«':'ğ‘€—ğ‘€½','á¹…e':'ğ‘€—ğ‘‚','á¹…ai':'ğ‘€—ğ‘ƒ','á¹…o':'ğ‘€—ğ‘„','á¹…au':'ğ‘€—ğ‘…','á¹…aá¹ƒ':'ğ‘€—ğ‘€',\n",
    "'ca':'ğ‘€˜â€‹','cÄ':'ğ‘€˜ğ‘€¸','ci':'ğ‘€˜ğ‘€º','cÄ«':'ğ‘€˜ğ‘€»','cu':'ğ‘€˜ğ‘€¼','cÅ«':'ğ‘€˜ğ‘€½','ce':'ğ‘€˜ğ‘‚','cai':'ğ‘€˜ğ‘ƒ','co':'ğ‘€˜ğ‘„','cau':'ğ‘€˜ğ‘…','caá¹ƒ':'ğ‘€˜ğ‘€',\n",
    "'cha':'ğ‘€™â€‹','chÄ':'ğ‘€™ğ‘€¸','chi':'ğ‘€™ğ‘€º','chÄ«':'ğ‘€™ğ‘€»','chu':'ğ‘€™ğ‘€¼','chÅ«':'ğ‘€™ğ‘€½','che':'ğ‘€™ğ‘‚','chai':'ğ‘€™ğ‘ƒ','cho':'ğ‘€™ğ‘„','chau':'ğ‘€™ğ‘…','chaá¹ƒ':'ğ‘€™ğ‘€',\n",
    "'ja':'ğ‘€š','jÄ':'ğ‘€šğ‘€¸','ji':'ğ‘€šğ‘€º','jÄ«':'ğ‘€šğ‘€»','ju':'ğ‘€šğ‘€¼','jÅ«':'ğ‘€šğ‘€½','je':'ğ‘€šğ‘‚','jai':'ğ‘€šğ‘ƒ','jo':'ğ‘€šğ‘„','jau':'ğ‘€šğ‘…','jaá¹ƒ':'ğ‘€šğ‘€',\n",
    "'jha':'ğ‘€›â€‹','jhÄ':'ğ‘€›ğ‘€¸','jhi':'ğ‘€›ğ‘€º','jhÄ«':'ğ‘€›ğ‘€»','jhu':'ğ‘€›ğ‘€¼','jhÅ«':'ğ‘€›ğ‘€½','jhe':'ğ‘€›ğ‘‚','jhai':'ğ‘€›ğ‘ƒ','jho':'ğ‘€›ğ‘„','jhau':'ğ‘€›ğ‘…','jhaá¹ƒ':'ğ‘€›ğ‘€¸ğ‘€',\n",
    "'Ã±a':'ğ‘€œâ€‹','Ã±Ä':'ğ‘€œğ‘€¸','Ã±i':'ğ‘€œğ‘€º','Ã±Ä«':'ğ‘€œğ‘€»','Ã±u':'ğ‘€œğ‘€¼','Ã±Å«':'ğ‘€œğ‘€½','Ã±e':'ğ‘€œğ‘‚','Ã±ai':'ğ‘€œğ‘ƒ','Ã±o':'ğ‘€œğ‘„','Ã±au':'ğ‘€œğ‘…','Ã±aá¹ƒ':'ğ‘€œğ‘€',\n",
    "'á¹­a':'ğ‘€â€‹','á¹­Ä':'ğ‘€ğ‘€¸','á¹­i':'ğ‘€ğ‘€º','á¹­Ä«':'ğ‘€ğ‘€»','á¹­u':'ğ‘€ğ‘€¼','á¹­Å«':'ğ‘€ğ‘€½','á¹­e':'ğ‘€ğ‘‚','á¹­ai':'ğ‘€ğ‘ƒ','á¹­o':'ğ‘€ğ‘„','á¹­au':'ğ‘€ğ‘…','á¹­aá¹ƒ':'ğ‘€ğ‘€',\n",
    "'á¹­ha':'ğ‘€â€‹','á¹­hÄ':'ğ‘€ğ‘€¸','á¹­hi':'ğ‘€ğ‘€º','á¹­hÄ«':'ğ‘€ğ‘€»','á¹­hu':'ğ‘€ğ‘€¼','á¹­hÅ«':'ğ‘€ğ‘€½','á¹­he':'ğ‘€ğ‘‚','á¹­hai':'ğ‘€ğ‘ƒ','á¹­ho':'ğ‘€ğ‘„','á¹­hau':'ğ‘€ğ‘…','á¹­haá¹ƒ':'ğ‘€ğ‘€',\n",
    "'á¸a':'ğ‘€Ÿâ€‹','á¸Ä':'ğ‘€¤ğ‘€¸','á¸i':'ğ‘€Ÿğ‘€º','á¸Ä«':'ğ‘€Ÿğ‘€»','á¸u':'ğ‘€Ÿğ‘€¼','á¸Å«':'ğ‘€Ÿğ‘€½','á¸e':'ğ‘€Ÿğ‘‚','á¸ai':'ğ‘€Ÿğ‘ƒ','á¸o':'ğ‘€Ÿğ‘„','á¸au':'ğ‘€Ÿğ‘…','á¸aá¹ƒ':'ğ‘€Ÿğ‘€',\n",
    "'á¸ha':'ğ‘€ â€‹','á¸hÄ':'ğ‘€ ğ‘€¸','á¸hi':'ğ‘€ ğ‘€º','á¸hÄ«':'ğ‘€ ğ‘€»','á¸hu':'ğ‘€ ğ‘€¼','á¸hÅ«':'ğ‘€ ğ‘€½','á¸he':'ğ‘€ ğ‘‚','á¸hai':'ğ‘€ ğ‘ƒ','á¸ho':'ğ‘€ ğ‘„','á¸hau':'ğ‘€ ğ‘…','á¸haá¹ƒ':'ğ‘€ ğ‘€',\n",
    "'á¹‡a':'ğ‘€¡â€‹','á¹‡Ä':'ğ‘€¡ğ‘€¸','á¹‡i':'ğ‘€¡ğ‘€º','á¹‡Ä«':'ğ‘€¡ğ‘€»','á¹‡u':'ğ‘€¡ğ‘€¼','á¹‡Å«':'ğ‘€¡ğ‘€½','á¹‡e':'ğ‘€¡ğ‘‚','á¹‡ai':'ğ‘€¡ğ‘ƒ','á¹‡o':'ğ‘€¡ğ‘„','á¹‡au':'ğ‘€¡ğ‘…','á¹‡aá¹ƒ':'ğ‘€¡ğ‘€',\n",
    "'ta':'ğ‘€¢â€‹','tÄ':'ğ‘€¢ğ‘€¸','ti':'ğ‘€¢ğ‘€º','tÄ«':'ğ‘€¢ğ‘€»','tu':'ğ‘€¢ğ‘€¼','tÅ«':'ğ‘€¢ğ‘€½','te':'ğ‘€¢ğ‘‚','tai':'ğ‘€¢ğ‘ƒ','to':'ğ‘€¢ğ‘„','tau':'ğ‘€¢ğ‘…','taá¹ƒ':'ğ‘€¢ğ‘€',\n",
    "'tha':'ğ‘€£â€‹','thÄ':'ğ‘€£ğ‘€¸','thi':'ğ‘€£ğ‘€º','thÄ«':'ğ‘€£ğ‘€»','thu':'ğ‘€£ğ‘€¼','thÅ«':'ğ‘€£ğ‘€½','the':'ğ‘€£ğ‘‚','thai':'ğ‘€£ğ‘ƒ','tho':'ğ‘€£ğ‘„','thau':'ğ‘€£ğ‘…','thaá¹ƒ':'ğ‘€£ğ‘€',\n",
    "'da':'ğ‘€¤â€‹','dÄ':'ğ‘€¤ğ‘€¸','di':'ğ‘€¤ğ‘€º','dÄ«':'ğ‘€¤ğ‘€»','du':'ğ‘€¤ğ‘€¼','dÅ«':'ğ‘€¤ğ‘€½','de':'ğ‘€¤ğ‘‚','dai':'ğ‘€¤ğ‘ƒ','do':'ğ‘€¤ğ‘„','dau':'ğ‘€¤ğ‘…','daá¹ƒ':'ğ‘€¤ğ‘€',\n",
    "'dha':'ğ‘€¥â€‹','dhÄ':'ğ‘€¥ğ‘€¸','dhi':'ğ‘€¥ğ‘€º','dhÄ«':'ğ‘€¥ğ‘€»','dhu':'ğ‘€¥ğ‘€¼','dhÅ«':'ğ‘€¥ğ‘€½','dhe':'ğ‘€¥ğ‘‚','dhai':'ğ‘€¥ğ‘ƒ','dho':'ğ‘€¥ğ‘„','dhau':'ğ‘€¥ğ‘…','dhaá¹ƒ':'ğ‘€¥ğ‘€',\n",
    "'na':'ğ‘€¦â€‹','nÄ':'ğ‘€¦ğ‘€¸','ni':'ğ‘€¦ğ‘€º','nÄ«':'ğ‘€¦ğ‘€»','nu':'ğ‘€¦ğ‘€¼','nÅ«':'ğ‘€¦ğ‘€½','ne':'ğ‘€¦ğ‘‚','nai':'ğ‘€¦ğ‘ƒ','no':'ğ‘€¦ğ‘„','nau':'ğ‘€¦ğ‘…','naá¹ƒ':'ğ‘€¦ğ‘€',\n",
    "'pa':'ğ‘€§â€‹','pÄ':'ğ‘€§ğ‘€¸','pi':'ğ‘€§ğ‘€º','pÄ«':'ğ‘€§ğ‘€»','pu':'ğ‘€§ğ‘€¼','pÅ«':'ğ‘€§ğ‘€½','pe':'ğ‘€§ğ‘‚','pai':'ğ‘€§ğ‘ƒ','po':'ğ‘€§ğ‘„','pau':'ğ‘€§ğ‘…','paá¹ƒ':'ğ‘€§ğ‘€',\n",
    "'pha':'ğ‘€¨â€‹','phÄ':'ğ‘€¨ğ‘€¸','phi':'ğ‘€¨ğ‘€º','phÄ«':'ğ‘€¨ğ‘€»','phu':'ğ‘€¨ğ‘€¼','phÅ«':'ğ‘€¨ğ‘€½','phe':'ğ‘€¨ğ‘‚','phai':'ğ‘€¨ğ‘ƒ','pho':'ğ‘€¨ğ‘„','phau':'ğ‘€¨ğ‘…','phaá¹ƒ':'ğ‘€¨ğ‘€',\n",
    "'ba':'ğ‘€©â€‹','bÄ':'ğ‘€©ğ‘€¸','bi':'ğ‘€©ğ‘€º','bÄ«':'ğ‘€©ğ‘€»','bu':'ğ‘€©ğ‘€¼','bÅ«':'ğ‘€©ğ‘€½','be':'ğ‘€©ğ‘‚','bai':'ğ‘€©ğ‘ƒ','bo':'ğ‘€©ğ‘„','bau':'ğ‘€©ğ‘…','baá¹ƒ':'ğ‘€©ğ‘€',\n",
    "'bha':'ğ‘€ªâ€‹','bhÄ':'ğ‘€ªğ‘€¸','bhi':'ğ‘€ªğ‘€º','bhÄ«':'ğ‘€ªğ‘€»','bhu':'ğ‘€ªğ‘€¼','bhÅ«':'ğ‘€ªğ‘€½','bhe':'ğ‘€ªğ‘‚','bhai':'ğ‘€ªğ‘ƒ','bho':'ğ‘€ªğ‘„','bhau':'ğ‘€ªğ‘…','bhaá¹ƒ':'ğ‘€ªğ‘€',\n",
    "'ma':'ğ‘€«â€‹','mÄ':'ğ‘€«ğ‘€¸','mi':'ğ‘€«ğ‘€º','mÄ«':'ğ‘€«ğ‘€»','mu':'ğ‘€«ğ‘€¼','mÅ«':'ğ‘€«ğ‘€½','me':'ğ‘€«ğ‘‚','mai':'ğ‘€«ğ‘ƒ','mo':'ğ‘€«ğ‘„','mau':'ğ‘€«ğ‘…','maá¹ƒ':'ğ‘€«ğ‘€',\n",
    "'ya':'ğ‘€¬â€‹','yÄ':'ğ‘€¬ğ‘€¸','yi':'ğ‘€¬ğ‘€º','yÄ«':'ğ‘€¬ğ‘€»','yu':'ğ‘€¬ğ‘€¼','yÅ«':'ğ‘€¬ğ‘€½','ye':'ğ‘€¬ğ‘‚','yai':'ğ‘€¬ğ‘ƒ','yo':'ğ‘€¬ğ‘„','yau':'ğ‘€¬ğ‘…','yaá¹ƒ':'ğ‘€¬ğ‘€',\n",
    "'ra':'ğ‘€­â€‹','rÄ':'ğ‘€­â€‹','ri':'ğ‘€­ğ‘€º','rÄ«':'ğ‘€­ğ‘€»','ru':'ğ‘€­ğ‘€¼','rÅ«':'ğ‘€­ğ‘€½','re':'ğ‘€­ğ‘‚','rai':'ğ‘€­ğ‘ƒ','ro':'ğ‘€­ğ‘„','rau':'ğ‘€­ğ‘…','raá¹ƒ':'ğ‘€­ğ‘€',\n",
    "'la':'ğ‘€®â€‹','lÄ':'ğ‘€®ğ‘€¸','li':'ğ‘€®ğ‘€º','lÄ«':'ğ‘€®ğ‘€»','lu':'ğ‘€®ğ‘€¼','lÅ«':'ğ‘€®ğ‘€½','le':'ğ‘€®ğ‘‚','lai':'ğ‘€®ğ‘ƒ','lo':'ğ‘€®ğ‘„','lau':'ğ‘€®ğ‘…','laá¹ƒ':'ğ‘€®ğ‘€',\n",
    "'va':'ğ‘€¯â€‹','vÄ':'ğ‘€¯ğ‘€¸','vi':'ğ‘€¯ğ‘€º','vÄ«':'ğ‘€¯ğ‘€»','vu':'ğ‘€¯ğ‘€¼','vÅ«':'ğ‘€¯ğ‘€½','ve':'ğ‘€¯ğ‘‚','vai':'ğ‘€¯ğ‘ƒ','vo':'ğ‘€¯ğ‘„','vau':'ğ‘€¯ğ‘…','vaá¹ƒ':'ğ‘€¯ğ‘€',\n",
    "'Å›a':'ğ‘€°','Å›Ä':'ğ‘€°ğ‘€¸','Å›i':'ğ‘€°ğ‘€º','Å›Ä«':'ğ‘€°ğ‘€»','Å›u':'ğ‘€°ğ‘€¼','Å›Å«':'ğ‘€°ğ‘€½','Å›e':'ğ‘€°ğ‘‚','Å›ai':'ğ‘€°ğ‘ƒ','Å›o':'ğ‘€°ğ‘„','Å›au':'ğ‘€°ğ‘…','Å›aá¹ƒ':'ğ‘€°ğ‘€',\n",
    "'á¹£a':'ğ‘€±â€‹','á¹£Ä':'ğ‘€±ğ‘€¸','á¹£i':'ğ‘€±ğ‘€º','á¹£Ä«':'ğ‘€±ğ‘€»','á¹£u':'ğ‘€±ğ‘€¼','á¹£Å«':'ğ‘€±ğ‘€½','á¹£e':'ğ‘€±ğ‘‚','á¹£ai':'ğ‘€±ğ‘ƒ','á¹£o':'ğ‘€±ğ‘„','á¹£au':'ğ‘€±ğ‘…','á¹£aá¹ƒ':'ğ‘€±ğ‘€',\n",
    "'sa':'ğ‘€²â€‹','sÄ':'ğ‘€²ğ‘€¸','si':'ğ‘€²ğ‘€º','sÄ«':'ğ‘€²ğ‘€»','su':'ğ‘€²ğ‘€¼','sÅ«':'ğ‘€²ğ‘€½','se':'ğ‘€²ğ‘‚','sai':'ğ‘€²ğ‘ƒ','so':'ğ‘€²ğ‘„','sau':'ğ‘€²ğ‘…','saá¹ƒ':'ğ‘€²ğ‘€',\n",
    "'ha':'ğ‘€³â€‹','hÄ':'ğ‘€³ğ‘€¸','hi':'ğ‘€³ğ‘€º','hÄ«':'ğ‘€³ğ‘€»','hu':'ğ‘€³ğ‘€¼','hÅ«':'ğ‘€³ğ‘€½','he':'ğ‘€³ğ‘‚','hai':'ğ‘€³ğ‘ƒ','ho':'ğ‘€³ğ‘„','hau':'ğ‘€³ğ‘…','haá¹ƒ':'ğ‘€³ğ‘€'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f096a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation/lines/line1.jpeg\n",
      "Segmentation/characters/char1.png\n",
      "Segmentation/characters/char2.png\n",
      "Segmentation/characters/char3.png\n",
      "Segmentation/characters/char4.png\n"
     ]
    }
   ],
   "source": [
    "# Character Segmentation\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Dropout, Flatten\n",
    "import os\n",
    "from keras.models import model_from_json\n",
    "\n",
    "char_model = model_from_json(open('model_files/model_arch.json').read())\n",
    "\n",
    "char_model.load_weights('model_files/mobilenet_model_weights.h5')\n",
    "\n",
    "# char_model = keras.models.load_model('model_files/best_val_acc_model.h5')\n",
    "\n",
    "line_dir = 'Segmentation/lines/'\n",
    "\n",
    "final_pred = \"\"\n",
    "\n",
    "# def prepare(file):\n",
    "#     IMG_SIZE_X = 100\n",
    "#     IMG_SIZE_Y = 140\n",
    "#     img_array = cv.imread(file, cv.IMREAD_GRAYSCALE)\n",
    "#     new_array = cv.resize(img_array, (IMG_SIZE_X, IMG_SIZE_Y))\n",
    "#     return new_array.reshape(-1, 100, 140, 1)\n",
    "\n",
    "def findPeakRegions(vpp, divider=0.42):\n",
    "    threshold = (np.max(vpp)-np.min(vpp))/divider\n",
    "    peaks = []\n",
    "    peaks_index = []\n",
    "    for i, vppv in enumerate(vpp):\n",
    "        if vppv > threshold:\n",
    "            peaks.append([i, vppv])\n",
    "    return peaks\n",
    "\n",
    "kth_img = 1\n",
    "\n",
    "line_breaks = []\n",
    "\n",
    "for filename in os.listdir(line_dir):\n",
    "    print(line_dir+filename)\n",
    "    img = cv.imread(line_dir + filename, cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "    vProj = np.sum(img,0)\n",
    "\n",
    "    peaks = findPeakRegions(vProj)\n",
    "    \n",
    "    peaksIndex = np.array(peaks)[:,0].astype(int)\n",
    "\n",
    "    segmentedImg = np.copy(img)\n",
    "    r,c = segmentedImg.shape\n",
    "\n",
    "    for ci in range(c):\n",
    "        if ci in peaksIndex:\n",
    "            segmentedImg[:,ci] = 0     \n",
    "\n",
    "    vProjLines = np.sum(segmentedImg,0)\n",
    "    vProjLines = np.append(vProjLines,[0,0,0])\n",
    "    \n",
    "    chars = []\n",
    "    charsB = []\n",
    "    charsE = []\n",
    "\n",
    "    for ci in range(len(vProjLines)):\n",
    "        if vProjLines[ci]!=0 and vProjLines[ci-1]==0:\n",
    "            charsB.append(ci)\n",
    "        if vProjLines[ci]!=0 and vProjLines[ci+1]==0:\n",
    "            charsE.append(ci)\n",
    "\n",
    "    for i in range(len(charsB)):\n",
    "        chars.append(img[:,charsB[i]:charsE[i]]) \n",
    "        if len(chars[i][0]) != 0 : \n",
    "            cv.imwrite(f\"Segmentation/characters/char{kth_img}.png\",chars[i])\n",
    "            print(f\"Segmentation/characters/char{kth_img}.png\")\n",
    "            kth_img += 1\n",
    "    line_breaks.append(kth_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44e4ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation/characters/char1.png\n",
      "1/1 [==============================] - 0s 290ms/step\n",
      "ni\n",
      "Segmentation/characters/char2.png\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "ji\n",
      "Segmentation/characters/char3.png\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "khe\n",
      "Segmentation/characters/char4.png\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "nÄ\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Dropout, Flatten\n",
    "import os\n",
    "\n",
    "def prepare(file):\n",
    "    IMG_SIZE = 224\n",
    "    \n",
    "    image = cv.imread(file)\n",
    "    image_resized= cv.resize(image, (224,224))\n",
    "    image=np.expand_dims(image_resized,axis=0)\n",
    "    return image\n",
    "\n",
    "char_dir = 'Segmentation/characters/'\n",
    "\n",
    "num_chars = len([filename for filename in os.listdir(char_dir)])\n",
    "\n",
    "final_pred = \"\"\n",
    "\n",
    "for filename in os.listdir(char_dir):\n",
    "    if i in line_breaks:\n",
    "        final_pred += '\\n '\n",
    "#     img_path = f\"{char_dir}char{i+1}.png\"\n",
    "    img_path = char_dir+filename\n",
    "    print(img_path)\n",
    "    img = prepare(img_path)\n",
    "    pred = char_model.predict([img])\n",
    "    \n",
    "    pred = list(pred[0])\n",
    "    char_type = class_names[pred.index(max(pred))]\n",
    "    print(char_type)\n",
    "    final_pred += brahmi_dict[char_type]\n",
    "    final_pred += \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f3d9203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ğ‘€… ğ‘€©\\u200b ğ‘€³ğ‘€» ğ‘€³ğ‘€º ğ‘€“ğ‘€º ğ‘€“ğ‘€º ğ‘€©\\u200b ğ‘€“ğ‘€º ğ‘€‡ ğ‘€¡ğ‘€» ğ‘€­ğ‘€¼ ğ‘€­\\u200b ğ‘€²\\u200b ğ‘€“ğ‘€º ğ‘€³ğ‘€º ğ‘€“ğ‘€º ğ‘€®\\u200b ğ‘€“ğ‘€º ğ‘€“ğ‘€º ğ‘€“ ğ‘€‡ ğ‘€–\\u200b ğ‘€Ÿ\\u200b ğ‘€­\\u200b ğ‘€“ğ‘€º ğ‘€“ğ‘€º ğ‘€”ğ‘€¼ ğ‘€­\\u200b ğ‘€“ğ‘€º ğ‘€«ğ‘€½ ğ‘€®\\u200b ğ‘€³ğ‘€» ğ‘€®\\u200b ğ‘€­ğ‘€¼ ğ‘€“ğ‘€º ğ‘€­\\u200b ğ‘€©ğ‘€¼ ğ‘€¡ğ‘€º ğ‘€§ğ‘‚ ğ‘€“ğ‘€º '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75c90ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transliteration \n",
    "\n",
    "from aksharamukha import transliterate\n",
    "\n",
    "trans_text = transliterate.process('autodetect','Devanagari',final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ede59acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count_vals = Counter(trans_text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cdc69442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ğ‘€•\\u200b': 1100, '': 1})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5e9cca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤… à¤¬â€‹ à¤¹à¥€ à¤¹à¤¿ à¤•à¤¿ à¤•à¤¿ à¤¬â€‹ à¤•à¤¿ à¤‡ à¤£à¥€ à¤°à¥ à¤°â€‹ à¤¸â€‹ à¤•à¤¿ à¤¹à¤¿ à¤•à¤¿ à¤²â€‹ à¤•à¤¿ à¤•à¤¿ à¤• à¤‡ à¤˜â€‹ à¤¡â€‹ à¤°â€‹ à¤•à¤¿ à¤•à¤¿ à¤–à¥ à¤°â€‹ à¤•à¤¿ à¤®à¥‚ à¤²â€‹ à¤¹à¥€ à¤²â€‹ à¤°à¥ à¤•à¤¿ à¤°â€‹ à¤¬à¥ à¤£à¤¿ à¤ªà¥‡ à¤•à¤¿ \n"
     ]
    }
   ],
   "source": [
    "print(trans_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7b2ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "char_model = model_from_json(open('model_files/model_arch.json').read())\n",
    "\n",
    "char_model.load_weights('model_files/mobilenet_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d616539c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "ki\n"
     ]
    }
   ],
   "source": [
    "# Single Img\n",
    "\n",
    "img_path = 'Segmentation/char38.png'\n",
    "prepare(img_path)\n",
    "\n",
    "pred = char_model.predict([img])\n",
    "    \n",
    "pred = list(pred[0])\n",
    "char_type = class_names[pred.index(max(pred))]\n",
    "print(char_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
